{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 14:09:32.760397: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-18 14:09:32.769075: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739884172.778592    8236 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739884172.781613    8236 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-18 14:09:32.792543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = \"1\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantizers import Quantizer\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantize_config import QuantizeConfig\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantizers import LastValueQuantizer, MovingAverageQuantizer\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantize import quantize_annotate_layer, quantize_apply\n",
    "from tensorflow_model_optimization.quantization.keras import quantize_apply, quantize_scope\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.python.profiler import model_analyzer\n",
    "from tensorflow.python.profiler import option_builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Generate test data:\n",
    "X=np.random.rand(10000,5)\n",
    "y = np.sum(X, axis=1)\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedRangeQuantizer(Quantizer):\n",
    "    def build(self, tensor_shape, name, layer):\n",
    "        range_var = layer.add_weight(\n",
    "            name=name + '_range',\n",
    "            initializer=tf.keras.initializers.Constant(6.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        return {'range_var': range_var}\n",
    "\n",
    "    def __call__(self, inputs, training, weights, **kwargs):\n",
    "        return tf.keras.backend.clip(inputs, 0.0, weights['range_var'])\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomLayerQuantizeConfig(QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return [\n",
    "        (layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)),\n",
    "        (layer.bias,   LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)),\n",
    "    ]\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        #return []\n",
    "        return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        layer.activation = quantize_activations[0]\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return [MovingAverageQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)]\n",
    "        #return []\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-18 14:09:34.339513: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1739884174.340408    8236 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=(5,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='relu')\n",
    "])\n",
    "#quantize_model = tfmot.quantization.keras.quantize_model\n",
    "#model2 = quantize_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    quantize_annotate_layer(layers.Dense(1024, activation='relu', input_shape=(5,)), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(512, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(256, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(128, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(64, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(1, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    #layers.Dense(100, activation='relu', input_shape=(5,)),\n",
    "    #layers.Dense(1, activation='relu')\n",
    "])\n",
    "#with quantize_scope({'CustomLayerQuantizeConfig': CustomLayerQuantizeConfig}):\n",
    "#    quant_aware_model = quantize_apply(model)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=(5,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='relu')\n",
    "])\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "quant_aware_model = quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562\n",
      "(10000, 5)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739884177.109546    8348 service.cc:148] XLA service 0x7d9d853ebef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1739884177.109564    8348 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
      "2025-02-18 14:09:37.112888: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1739884177.122428    8348 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1739884177.168621    8348 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 3s 2ms/step - loss: 0.2073 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5.4538e-05 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2.7801e-05 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.7523e-05 - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 1s 7ms/step - loss: 1.4200e-05 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 6.7275e-04 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4.1889e-04 - accuracy: 0.0000e+00\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1092 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 9.3200e-04 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 7.0756e-04 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 4.3278e-04 - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.9820e-04 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 2.7748e-04 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 4.7070e-04 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 5.7942e-04 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 7.7235e-04 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 9.7533e-04 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7d9f282a1180>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import quantize_model\n",
    "epochs=10\n",
    "batch_size=64\n",
    "num_training_samples=len(X)\n",
    "print(int(num_training_samples/batch_size*epochs))\n",
    "model2, callbacks=quantize_model.get_pruning_wrapper(model2, 0.5, epochs,batch_size,num_training_samples)\n",
    "model2.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "print(np.shape(X))\n",
    "\n",
    "model2.fit(X, y,callbacks=callbacks, epochs=epochs, batch_size=batch_size)\n",
    "quant_aware_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "quant_aware_model.fit(X, y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = quantize_model.strip_prune(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "3.1012395830769353\n",
      "[[3.1076546]]\n",
      "[[3.1029394]]\n"
     ]
    }
   ],
   "source": [
    "vals=np.random.rand(1,5)\n",
    "result=np.sum(vals)\n",
    "prediction=quant_aware_model.predict(vals)\n",
    "prediction2=model2.predict(vals)\n",
    "print(result)\n",
    "print(prediction)\n",
    "print(prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(X.shape[0], 100, replace=False)\n",
    "x_random = X[index]\n",
    "def representative_data_gen():\n",
    "    # Here, let's use 100 samples for calibration\n",
    "    for i in range(100):\n",
    "        # The model expects (batch_size=1, 5) if it’s Dense(…, input_shape=(5,)).\n",
    "        # So we add a batch dimension of size 1:\n",
    "        yield [x_random[i:i+1].astype(np.float32)]  # shape (1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8gux02z_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp8gux02z_/assets\n",
      "/home/henrik/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1739884188.701715    8236 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739884188.701724    8236 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-18 14:09:48.701876: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp8gux02z_\n",
      "2025-02-18 14:09:48.702294: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-02-18 14:09:48.702301: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp8gux02z_\n",
      "I0000 00:00:1739884188.705277    8236 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2025-02-18 14:09:48.705634: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-02-18 14:09:48.719052: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp8gux02z_\n",
      "2025-02-18 14:09:48.722592: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 20718 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpot3zqgwz/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpot3zqgwz/assets\n",
      "/home/henrik/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1739884189.723732    8236 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739884189.723742    8236 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-18 14:09:49.723837: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpot3zqgwz\n",
      "2025-02-18 14:09:49.725528: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-02-18 14:09:49.725536: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpot3zqgwz\n",
      "2025-02-18 14:09:49.736766: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-02-18 14:09:49.786227: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpot3zqgwz\n",
      "2025-02-18 14:09:49.798485: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 74649 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "quantconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "\n",
    "quantize_model.quantize_8_bit(model2,x_random, \"testfolder/32bit\")\n",
    "\n",
    "\n",
    "quantconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "quantconverter.representative_dataset = representative_data_gen\n",
    "quantlite=quantconverter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714320\n",
      "760976\n"
     ]
    }
   ],
   "source": [
    "#quant_aware_model.save('testfolder/8bit.h5')  # Save the model in HDF5 format\n",
    "#model2.save('testfolder/32bit.h5')  # Save the model in HDF5 format\n",
    "with open(\"testfolder/8bit.tflite\", \"wb\") as f:\n",
    "    f.write(quantlite)\n",
    "\n",
    "# Get the size of the saved model file in bytes\n",
    "model_size = os.path.getsize('testfolder/8bit.tflite')\n",
    "print(model_size)\n",
    "\n",
    "model_size = os.path.getsize('testfolder/32bit.tflite')\n",
    "print(model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:\n",
    "714344\n",
    "761032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "\n",
    "with open(\"testfolder/8bit.tflite\", \"rb\") as f_in:\n",
    "    model_data = f_in.read()\n",
    "\n",
    "compressed_data = lzma.compress(model_data)\n",
    "\n",
    "with open(\"testfolder/8bit.tflite.xz\", \"wb\") as f_out:\n",
    "    f_out.write(compressed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testfolder/32bit.tflite\", \"rb\") as f_in:\n",
    "    model_data = f_in.read()\n",
    "\n",
    "compressed_data = lzma.compress(model_data)\n",
    "\n",
    "with open(\"testfolder/32bit.tflite.xz\", \"wb\") as f_out:\n",
    "    f_out.write(compressed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
