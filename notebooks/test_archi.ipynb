{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEPS =4\n",
    "def create_sequences(X, y, time_steps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        X_seq.append(X[i : i + time_steps])  # Collect time_steps observations\n",
    "        y_seq.append(y[i + time_steps])  # Predict the next value (shifted target)\n",
    "    return np.array(X_seq), np.array(y_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0             0\n",
      "timestamp              0\n",
      "sensor_00          10208\n",
      "sensor_01            369\n",
      "sensor_02             19\n",
      "sensor_03             19\n",
      "sensor_04             19\n",
      "sensor_05             19\n",
      "sensor_06           4798\n",
      "sensor_07           5451\n",
      "sensor_08           5107\n",
      "sensor_09           4595\n",
      "sensor_10             19\n",
      "sensor_11             19\n",
      "sensor_12             19\n",
      "sensor_13             19\n",
      "sensor_14             21\n",
      "sensor_15         220320\n",
      "sensor_16             31\n",
      "sensor_17             46\n",
      "sensor_18             46\n",
      "sensor_19             16\n",
      "sensor_20             16\n",
      "sensor_21             16\n",
      "sensor_22             41\n",
      "sensor_23             16\n",
      "sensor_24             16\n",
      "sensor_25             36\n",
      "sensor_26             20\n",
      "sensor_27             16\n",
      "sensor_28             16\n",
      "sensor_29             72\n",
      "sensor_30            261\n",
      "sensor_31             16\n",
      "sensor_32             68\n",
      "sensor_33             16\n",
      "sensor_34             16\n",
      "sensor_35             16\n",
      "sensor_36             16\n",
      "sensor_37             16\n",
      "sensor_38             27\n",
      "sensor_39             27\n",
      "sensor_40             27\n",
      "sensor_41             27\n",
      "sensor_42             27\n",
      "sensor_43             27\n",
      "sensor_44             27\n",
      "sensor_45             27\n",
      "sensor_46             27\n",
      "sensor_47             27\n",
      "sensor_48             27\n",
      "sensor_49             27\n",
      "sensor_50          77017\n",
      "sensor_51          15383\n",
      "machine_status         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"datasets/sensor.csv\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_203727/2929346405.py:8: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[sensor_cols] = df[sensor_cols].fillna(method='ffill')\n",
      "/tmp/ipykernel_203727/2929346405.py:9: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[sensor_cols] = df[sensor_cols].fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "sensors_to_drop = ['Unnamed: 0', 'timestamp','sensor_15', 'sensor_50']\n",
    "df = df.drop(columns=sensors_to_drop)\n",
    "\n",
    "sensor_cols = df.columns[df.isnull().any()].tolist()\n",
    "df[sensor_cols] = df[sensor_cols].interpolate(method='linear')\n",
    "\n",
    "# If any remaining NaNs, use forward/backward fill\n",
    "df[sensor_cols] = df[sensor_cols].fillna(method='ffill')\n",
    "df[sensor_cols] = df[sensor_cols].fillna(method='bfill')\n",
    "# Verify no missing values remain\n",
    "broken_positions = df.index[df[\"machine_status\"] == \"BROKEN\"].tolist()\n",
    "\n",
    "# Assign \"BROKEN\" to 59 previous points\n",
    "for pos in broken_positions:\n",
    "    start = max(0, pos - 10)  # Ensure we don't go below index 0\n",
    "    df.loc[start:pos, \"machine_status\"] = \"BROKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[\"machine_status\"]\n",
    "X=df.drop(columns=['machine_status'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.7, shuffle=False)\n",
    "scaler=MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, TIME_STEPS)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, TIME_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(220320, 51)\n",
      "{'BROKEN': 77, 'NORMAL': 205766, 'RECOVERING': 14477}\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(df))\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "# Display results\n",
    "category_counts = dict(zip(unique, counts))\n",
    "print(category_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    NORMAL\n",
      "1    NORMAL\n",
      "2    NORMAL\n",
      "3    NORMAL\n",
      "4    NORMAL\n",
      "Name: machine_status, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['BROKEN' 'NORMAL' 'RECOVERING']\n"
     ]
    }
   ],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y_train_seq = label_encoder.fit_transform(y_train_seq)\n",
    "y_test_seq = label_encoder.transform(y_test_seq)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "num_classes = len(np.unique(y_train_seq))\n",
    "print(num_classes)\n",
    "print(np.unique(y_test))\n",
    "y_train_seq = to_categorical(y_train_seq, num_classes)\n",
    "y_test_seq = to_categorical(y_test_seq, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.]\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(y_train_seq))\n",
    "print(np.argmax(y_train_seq, axis=1))\n",
    "y_train_classes = np.argmax(y_train_seq, axis=1)\n",
    "\n",
    "# Compute class weights\n",
    "class_labels = np.unique(y_train_classes)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=class_labels, y=y_train_classes)\n",
    "\n",
    "# Convert to a dictionary for Keras\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow.keras.backend as K\n",
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, K.floatx())\n",
    "        cross_entropy = K.binary_crossentropy(y_true, y_pred)\n",
    "        weight = alpha * y_true + (1 - alpha) * (1 - y_true)\n",
    "        focal_loss = weight * K.pow(1 - y_pred, gamma) * cross_entropy\n",
    "        return K.mean(focal_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_sampler(X, y):\n",
    "    \"\"\" Custom sampler to favor underrepresented classes \"\"\"\n",
    "    threshold=500\n",
    "    rare_classes = [cls for cls in np.unique(y) if np.sum(y == cls) < threshold]\n",
    "    \n",
    "    while True:\n",
    "        indices = []\n",
    "        for cls in rare_classes:\n",
    "            indices.extend(np.random.choice(np.where(y == cls)[0], size=10, replace=True))\n",
    "        indices.extend(np.random.choice(len(y), size=22))  # Add some general samples\n",
    "        np.random.shuffle(indices)\n",
    "        yield X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_features = X_train.shape[1]\n",
    "inputs = Input(shape=(TIME_STEPS, num_features))\n",
    "x = layers.Conv1D(64, kernel_size=2, padding=\"same\", activation=\"relu\")(inputs)\n",
    "filters_list=[192,224,128,160, 160]\n",
    "for i in range(3):\n",
    "    shortcut = x  # save input for the skip connection      \n",
    "    # First convolution in block\n",
    "    y = layers.Conv1D(filters_list[i], kernel_size=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    filter2=filters_list[i]\n",
    "    # Second convolution in block (no activation until after adding the shortcut)\n",
    "    y = layers.Conv1D(filter2, kernel_size=2, padding=\"same\")(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "\n",
    "    # If the number of channels does not match, adjust the shortcut\n",
    "    if shortcut.shape[-1] != filter2:\n",
    "        shortcut = layers.Conv1D(filter2, kernel_size=1, padding=\"same\")(shortcut)\n",
    "    # Add the shortcut (residual connection)\n",
    "    x = layers.Add()([shortcut, y])\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "# Global pooling and output classification layer\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "# Compile the model\n",
    "model = models.Model(inputs, outputs)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)  # Set LR here\n",
    "model.compile(optimizer=optimizer, \n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "class PerClassAccuracyCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_data, class_names=None):\n",
    "        \"\"\"\n",
    "        Callback to compute per-class accuracy after each epoch.\n",
    "        \n",
    "        val_data: (X_val, y_val) validation dataset\n",
    "        class_names: List of class names for logging\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.X_val, self.y_val = val_data\n",
    "        self.class_names = class_names if class_names else np.unique(self.y_val)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Get model predictions\n",
    "        y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)  # Convert softmax to class labels\n",
    "        y_true_classes = np.argmax(self.y_val, axis=1)  # True class labels\n",
    "\n",
    "        # Compute per-class accuracy\n",
    "        class_accuracies = {}\n",
    "        for class_idx in np.unique(y_true_classes):\n",
    "            mask = (y_true_classes == class_idx)\n",
    "            acc = accuracy_score(y_true_classes[mask], y_pred_classes[mask])\n",
    "            class_name = self.class_names[class_idx] if isinstance(self.class_names, list) else class_idx\n",
    "            class_accuracies[class_name] = acc\n",
    "\n",
    "        # Log per-class accuracy\n",
    "        #print(f\"\\nEpoch {epoch+1} Per-Class Accuracy:\")\n",
    "        #for class_name, acc in class_accuracies.items():\n",
    "        #    print(f\"  - {class_name}: {acc:.4f}\")\n",
    "\n",
    "        logs = logs or {}\n",
    "        logs.update(class_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m489/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9464 - loss: 0.1423"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 16:33:46.597804: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2033', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9487 - loss: 0.1363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 16:33:49.456415: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_263', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 17ms/step - accuracy: 0.9488 - loss: 0.1361 - val_accuracy: 0.9845 - val_loss: 0.0526 - BROKEN: 0.0000e+00 - NORMAL: 0.9976 - RECOVERING: 0.8097\n",
      "Epoch 2/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 0.9382 - val_loss: 0.1256 - BROKEN: 0.0182 - NORMAL: 0.9344 - RECOVERING: 0.9952\n",
      "Epoch 3/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 0.0014 - val_accuracy: 0.9848 - val_loss: 0.0473 - BROKEN: 0.0182 - NORMAL: 0.9864 - RECOVERING: 0.9674\n",
      "Epoch 4/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9994 - loss: 0.0015 - val_accuracy: 0.9828 - val_loss: 0.0621 - BROKEN: 0.0727 - NORMAL: 0.9823 - RECOVERING: 0.9954\n",
      "Epoch 5/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 0.0015 - val_accuracy: 0.9693 - val_loss: 0.0924 - BROKEN: 0.0727 - NORMAL: 0.9677 - RECOVERING: 0.9963\n",
      "Epoch 6/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 0.0013 - val_accuracy: 0.9838 - val_loss: 0.0603 - BROKEN: 0.0727 - NORMAL: 0.9876 - RECOVERING: 0.9361\n",
      "Epoch 7/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 8.5697e-04 - val_accuracy: 0.9493 - val_loss: 0.1458 - BROKEN: 0.0000e+00 - NORMAL: 0.9461 - RECOVERING: 0.9980\n",
      "Epoch 8/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9995 - loss: 0.0014 - val_accuracy: 0.9827 - val_loss: 0.0638 - BROKEN: 0.0727 - NORMAL: 0.9822 - RECOVERING: 0.9948\n",
      "Epoch 9/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 5.0339e-04 - val_accuracy: 0.9266 - val_loss: 0.3201 - BROKEN: 0.0727 - NORMAL: 0.9218 - RECOVERING: 0.9976\n",
      "Epoch 10/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9996 - loss: 0.0010 - val_accuracy: 0.9618 - val_loss: 0.1080 - BROKEN: 0.0727 - NORMAL: 0.9599 - RECOVERING: 0.9930\n",
      "Epoch 11/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 7.1223e-04 - val_accuracy: 0.9774 - val_loss: 0.0825 - BROKEN: 0.0727 - NORMAL: 0.9767 - RECOVERING: 0.9915\n",
      "Epoch 12/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 4.5371e-04 - val_accuracy: 0.9724 - val_loss: 0.1153 - BROKEN: 0.0727 - NORMAL: 0.9708 - RECOVERING: 0.9987\n",
      "Epoch 13/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9997 - loss: 7.6356e-04 - val_accuracy: 0.9895 - val_loss: 0.0752 - BROKEN: 0.0727 - NORMAL: 0.9895 - RECOVERING: 0.9944\n",
      "Epoch 14/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 7.5562e-04 - val_accuracy: 0.9856 - val_loss: 0.0771 - BROKEN: 0.0364 - NORMAL: 0.9852 - RECOVERING: 0.9967\n",
      "Epoch 15/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 6.7311e-04 - val_accuracy: 0.9821 - val_loss: 0.0862 - BROKEN: 0.0909 - NORMAL: 0.9816 - RECOVERING: 0.9941\n",
      "Epoch 16/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 4.5772e-04 - val_accuracy: 0.9919 - val_loss: 0.0692 - BROKEN: 0.0364 - NORMAL: 0.9928 - RECOVERING: 0.9846\n",
      "Epoch 17/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 6.4606e-04 - val_accuracy: 0.9374 - val_loss: 0.2803 - BROKEN: 0.0727 - NORMAL: 0.9333 - RECOVERING: 0.9981\n",
      "Epoch 18/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 3.9581e-04 - val_accuracy: 0.9785 - val_loss: 0.0960 - BROKEN: 0.0727 - NORMAL: 0.9777 - RECOVERING: 0.9941\n",
      "Epoch 19/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9998 - loss: 4.7072e-04 - val_accuracy: 0.9836 - val_loss: 0.0882 - BROKEN: 0.0727 - NORMAL: 0.9840 - RECOVERING: 0.9824\n",
      "Epoch 20/20\n",
      "\u001b[1m517/517\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9999 - loss: 2.7797e-04 - val_accuracy: 0.9233 - val_loss: 0.4621 - BROKEN: 0.0727 - NORMAL: 0.9183 - RECOVERING: 0.9977\n"
     ]
    }
   ],
   "source": [
    "class_names = ['BROKEN', 'NORMAL', 'RECOVERING']  # Replace with actual class labels\n",
    "\n",
    "# Add callback to model training\n",
    "per_class_acc_callback = PerClassAccuracyCallback((X_test_seq, y_test_seq), class_names)\n",
    "history =model.fit(X_train_seq, y_train_seq, epochs=20, validation_data=(X_test_seq, y_test_seq), batch_size=128, callbacks=[per_class_acc_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "#loss, metric = model.evaluate(X_test_seq, y_test_seq)\n",
    "#print(f\"Test Loss: {loss:.4f}, Test {'Accuracy' if 'num_classes' in locals() else 'MAE'}: {metric:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#y_pred=model.predict(X_test_seq)\n",
    "#y_hat = np.argmax(y_pred, axis=1)  # Get the highest probability class index\n",
    "#y_true = np.argmax(y_test_seq, axis=1)  # Ensure y_true is also class indices\n",
    "#print(y_true)\n",
    "#print(y_hat)\n",
    "\n",
    "#cm = confusion_matrix(y_true, y_hat)\n",
    "\n",
    "# Plot using seaborn\n",
    "#plt.figure(figsize=(8, 6))\n",
    "#sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(num_classes), yticklabels=range(num_classes))\n",
    "#plt.xlabel(\"Predicted Label\")\n",
    "#plt.ylabel(\"True Label\")\n",
    "#plt.title(\"Confusion Matrix\")\n",
    "#plt.gca().invert_yaxis()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Good start: 3 timesteps, 30 minute broken window. 99% acc, 4 (0,0)\n",
    "#Good start: 2 timesteps, 30 minute broken window. 98% acc, 5 (0,0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
