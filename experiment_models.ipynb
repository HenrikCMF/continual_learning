{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = \"1\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantizers import Quantizer\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantize_config import QuantizeConfig\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantizers import LastValueQuantizer, MovingAverageQuantizer\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantize import quantize_annotate_layer, quantize_apply\n",
    "from tensorflow_model_optimization.quantization.keras import quantize_apply, quantize_scope\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.python.profiler import model_analyzer\n",
    "from tensorflow.python.profiler import option_builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Generate test data:\n",
    "X=np.random.rand(10000,5)\n",
    "y = np.sum(X, axis=1)\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedRangeQuantizer(Quantizer):\n",
    "    def build(self, tensor_shape, name, layer):\n",
    "        range_var = layer.add_weight(\n",
    "            name=name + '_range',\n",
    "            initializer=tf.keras.initializers.Constant(6.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        return {'range_var': range_var}\n",
    "\n",
    "    def __call__(self, inputs, training, weights, **kwargs):\n",
    "        return tf.keras.backend.clip(inputs, 0.0, weights['range_var'])\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomLayerQuantizeConfig(QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return [\n",
    "        (layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)),\n",
    "        (layer.bias,   LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)),\n",
    "    ]\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        #return []\n",
    "        return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        layer.activation = quantize_activations[0]\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return [MovingAverageQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)]\n",
    "        #return []\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=(5,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    quantize_annotate_layer(layers.Dense(1024, activation='relu', input_shape=(5,)), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(512, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(256, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(128, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(64, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(1, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    #layers.Dense(100, activation='relu', input_shape=(5,)),\n",
    "    #layers.Dense(1, activation='relu')\n",
    "])\n",
    "with quantize_scope({'CustomLayerQuantizeConfig': CustomLayerQuantizeConfig}):\n",
    "    quant_aware_model = quantize_apply(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562\n",
      "(10000, 5)\n",
      "Epoch 1/10\n",
      "157/157 [==============================] - 1s 2ms/step - loss: 0.2113 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4.2793e-05 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.8984e-05 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 1.2721e-05 - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 1.1960e-05 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 2.0575e-04 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 0.0044 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 2.4677e-04 - accuracy: 0.0000e+00\n",
      "Epoch 1/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0991 - accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0016 - accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0014 - accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 8.7190e-04 - accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 9.2012e-04 - accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 6.3293e-04 - accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 7.1729e-04 - accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0010 - accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0014 - accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0015 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7ef3d6b27340>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import quantize_model\n",
    "epochs=10\n",
    "batch_size=64\n",
    "num_training_samples=len(X)\n",
    "print(int(num_training_samples/batch_size*epochs))\n",
    "model2, callbacks=quantize_model.get_pruning_wrapper(model2, 0.5, epochs,batch_size,num_training_samples)\n",
    "model2.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "print(np.shape(X))\n",
    "\n",
    "model2.fit(X, y,callbacks=callbacks, epochs=epochs, batch_size=batch_size)\n",
    "quant_aware_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "quant_aware_model.fit(X, y, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = quantize_model.strip_prune(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "2.315572615813018\n",
      "[[2.3326743]]\n",
      "[[2.3185055]]\n"
     ]
    }
   ],
   "source": [
    "vals=np.random.rand(1,5)\n",
    "result=np.sum(vals)\n",
    "prediction=quant_aware_model.predict(vals)\n",
    "prediction2=model2.predict(vals)\n",
    "print(result)\n",
    "print(prediction)\n",
    "print(prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(X.shape[0], 100, replace=False)\n",
    "x_random = X[index]\n",
    "def representative_data_gen():\n",
    "    # Here, let's use 100 samples for calibration\n",
    "    for i in range(100):\n",
    "        # The model expects (batch_size=1, 5) if it’s Dense(…, input_shape=(5,)).\n",
    "        # So we add a batch dimension of size 1:\n",
    "        yield [x_random[i:i+1].astype(np.float32)]  # shape (1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9smdjlsg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9smdjlsg/assets\n",
      "/home/henrik/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1739730060.506600   45954 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739730060.506611   45954 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-16 19:21:00.506698: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp9smdjlsg\n",
      "2025-02-16 19:21:00.507052: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-02-16 19:21:00.507060: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp9smdjlsg\n",
      "2025-02-16 19:21:00.509150: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-02-16 19:21:00.521249: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp9smdjlsg\n",
      "2025-02-16 19:21:00.524801: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 18103 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpjfo6m8fm/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpjfo6m8fm/assets\n",
      "/home/henrik/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1739730062.158329   45954 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739730062.158339   45954 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-16 19:21:02.158430: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpjfo6m8fm\n",
      "2025-02-16 19:21:02.160814: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-02-16 19:21:02.160822: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpjfo6m8fm\n",
      "2025-02-16 19:21:02.176247: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-02-16 19:21:02.240096: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpjfo6m8fm\n",
      "2025-02-16 19:21:02.256697: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 98268 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "quantconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "\n",
    "quantize_model.quantize_8_bit(model2,x_random, \"testfolder/32bit\")\n",
    "\n",
    "\n",
    "quantconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "quantconverter.representative_dataset = representative_data_gen\n",
    "quantlite=quantconverter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714344\n",
      "761032\n"
     ]
    }
   ],
   "source": [
    "#quant_aware_model.save('testfolder/8bit.h5')  # Save the model in HDF5 format\n",
    "#model2.save('testfolder/32bit.h5')  # Save the model in HDF5 format\n",
    "with open(\"testfolder/8bit.tflite\", \"wb\") as f:\n",
    "    f.write(quantlite)\n",
    "\n",
    "# Get the size of the saved model file in bytes\n",
    "model_size = os.path.getsize('testfolder/8bit.tflite')\n",
    "print(model_size)\n",
    "\n",
    "model_size = os.path.getsize('testfolder/32bit.tflite')\n",
    "print(model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before:\n",
    "714344\n",
    "761032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "\n",
    "with open(\"testfolder/8bit.tflite\", \"rb\") as f_in:\n",
    "    model_data = f_in.read()\n",
    "\n",
    "compressed_data = lzma.compress(model_data)\n",
    "\n",
    "with open(\"testfolder/8bit.tflite.xz\", \"wb\") as f_out:\n",
    "    f_out.write(compressed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"testfolder/32bit.tflite\", \"rb\") as f_in:\n",
    "    model_data = f_in.read()\n",
    "\n",
    "compressed_data = lzma.compress(model_data)\n",
    "\n",
    "with open(\"testfolder/32bit.tflite.xz\", \"wb\") as f_out:\n",
    "    f_out.write(compressed_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
