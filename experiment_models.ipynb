{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = \"1\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantizers import Quantizer\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantize_config import QuantizeConfig\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantizers import LastValueQuantizer, MovingAverageQuantizer\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.quantize import quantize_annotate_layer, quantize_apply\n",
    "from tensorflow_model_optimization.quantization.keras import quantize_apply, quantize_scope\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.python.profiler import model_analyzer\n",
    "from tensorflow.python.profiler import option_builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 5)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Generate test data:\n",
    "X=np.random.rand(10000,5)\n",
    "y = np.sum(X, axis=1)\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedRangeQuantizer(Quantizer):\n",
    "    def build(self, tensor_shape, name, layer):\n",
    "        range_var = layer.add_weight(\n",
    "            name=name + '_range',\n",
    "            initializer=tf.keras.initializers.Constant(6.0),\n",
    "            trainable=False\n",
    "        )\n",
    "        return {'range_var': range_var}\n",
    "\n",
    "    def __call__(self, inputs, training, weights, **kwargs):\n",
    "        return tf.keras.backend.clip(inputs, 0.0, weights['range_var'])\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CustomLayerQuantizeConfig(QuantizeConfig):\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "        return [\n",
    "        (layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)),\n",
    "        (layer.bias,   LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)),\n",
    "    ]\n",
    "\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "        #return []\n",
    "        return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "        layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "        layer.activation = quantize_activations[0]\n",
    "\n",
    "    def get_output_quantizers(self, layer):\n",
    "        return [MovingAverageQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False)]\n",
    "        #return []\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    layers.Dense(1024, activation='relu', input_shape=(5,)),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    quantize_annotate_layer(layers.Dense(1024, activation='relu', input_shape=(5,)), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(512, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(256, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(128, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(64, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    quantize_annotate_layer(layers.Dense(1, activation='relu'), quantize_config=CustomLayerQuantizeConfig()),\n",
    "    #layers.Dense(100, activation='relu', input_shape=(5,)),\n",
    "    #layers.Dense(1, activation='relu')\n",
    "])\n",
    "with quantize_scope({'CustomLayerQuantizeConfig': CustomLayerQuantizeConfig}):\n",
    "    quant_aware_model = quantize_apply(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 3ms/step - loss: 0.0948 - accuracy: 0.0000e+00\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.2843 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7f7414303a60>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_aware_model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "quant_aware_model.fit(X, y, epochs=1)\n",
    "model2.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model2.fit(X, y, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f74a49c7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f74a49c7400> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step\n",
      "1.8890255552311528\n",
      "[[1.858444]]\n"
     ]
    }
   ],
   "source": [
    "vals=np.random.rand(1,5)\n",
    "result=np.sum(vals)\n",
    "prediction=quant_aware_model.predict(vals)\n",
    "print(result)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.random.choice(X.shape[0], 100, replace=False)\n",
    "x_random = X[index]\n",
    "def representative_data_gen():\n",
    "    # Here, let's use 100 samples for calibration\n",
    "    for i in range(100):\n",
    "        # The model expects (batch_size=1, 5) if it’s Dense(…, input_shape=(5,)).\n",
    "        # So we add a batch dimension of size 1:\n",
    "        yield [x_random[i:i+1].astype(np.float32)]  # shape (1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpsdvggxie/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpsdvggxie/assets\n",
      "/home/henrik/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1739553500.319659   27572 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739553500.319670   27572 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-14 18:18:20.319754: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpsdvggxie\n",
      "2025-02-14 18:18:20.321804: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-02-14 18:18:20.321812: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpsdvggxie\n",
      "2025-02-14 18:18:20.337533: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-02-14 18:18:20.400236: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpsdvggxie\n",
      "2025-02-14 18:18:20.415983: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 96229 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxo32gul4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxo32gul4/assets\n",
      "/home/henrik/anaconda3/envs/tf-gpu/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1739553500.895669   27572 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739553500.895680   27572 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-14 18:18:20.895787: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpxo32gul4\n",
      "2025-02-14 18:18:20.896483: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-02-14 18:18:20.896491: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpxo32gul4\n",
      "2025-02-14 18:18:20.901250: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-02-14 18:18:20.931064: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpxo32gul4\n",
      "2025-02-14 18:18:20.937526: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 41740 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    }
   ],
   "source": [
    "quantconverter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter2 = tf.lite.TFLiteConverter.from_keras_model(model2)\n",
    "\n",
    "converter2.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter2.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter2.representative_dataset = representative_data_gen\n",
    "\n",
    "quantconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "quantconverter.representative_dataset = representative_data_gen\n",
    "quantlite=quantconverter.convert()\n",
    "lite2=converter2.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714344\n",
      "761032\n"
     ]
    }
   ],
   "source": [
    "#quant_aware_model.save('testfolder/8bit.h5')  # Save the model in HDF5 format\n",
    "#model2.save('testfolder/32bit.h5')  # Save the model in HDF5 format\n",
    "with open(\"testfolder/8bit.tflite\", \"wb\") as f:\n",
    "    f.write(quantlite)\n",
    "\n",
    "with open(\"testfolder/32bit.tflite\", \"wb\") as f:\n",
    "    f.write(lite2)\n",
    "\n",
    "# Get the size of the saved model file in bytes\n",
    "model_size = os.path.getsize('testfolder/8bit.tflite')\n",
    "print(model_size)\n",
    "\n",
    "model_size = os.path.getsize('testfolder/32bit.tflite')\n",
    "print(model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma\n",
    "\n",
    "with open(\"testfolder/8bit.tflite\", \"rb\") as f_in:\n",
    "    model_data = f_in.read()\n",
    "\n",
    "compressed_data = lzma.compress(model_data)\n",
    "\n",
    "with open(\"testfolder/8bit.tflite.xz\", \"wb\") as f_out:\n",
    "    f_out.write(compressed_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
